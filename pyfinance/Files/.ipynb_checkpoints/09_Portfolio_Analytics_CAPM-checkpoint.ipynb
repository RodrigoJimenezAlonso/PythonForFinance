{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065edf70",
   "metadata": {},
   "source": [
    "# Portfolio Analytics, Sharpe Ratio, Optimization, Order Books, Short Selling, and CAPM — Self‑Guided Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc9ba6",
   "metadata": {},
   "source": [
    "\n",
    "These notes are **self-contained** and runnable **offline**. You’ll simulate price series, compute portfolio statistics, build a Sharpe‑ratio Monte Carlo, (optionally) solve an **optimization** with SciPy (falls back gracefully if SciPy isn’t available), draw the **efficient frontier**, and review market microstructure (order books, HFT), **short selling**, **CAPM**, and **stock splits/dividends**.  \n",
    "Each section follows a pattern: **What & Why → Steps → Worked Example → Checks → Exercise (Solved).**\n",
    "\n",
    "> Tip: Run cells **top‑to‑bottom**. No internet or external data is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6156b",
   "metadata": {},
   "source": [
    "## 0) Setup — imports, reproducibility, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d226d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Matplotlib: one figure per chart; do not set styles or colors.\n",
    "def newfig():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "BUSINESS_DAYS_PER_YEAR = 252\n",
    "\n",
    "def annualize_return(daily_ret_mean):\n",
    "    \"\"\"Annualize a *mean daily* return (arithmetic).\"\"\"\n",
    "    return (1 + daily_ret_mean)**BUSINESS_DAYS_PER_YEAR - 1\n",
    "\n",
    "def annualize_vol(daily_ret_std):\n",
    "    \"\"\"Annualize a *daily* std (volatility).\"\"\"\n",
    "    return daily_ret_std * np.sqrt(BUSINESS_DAYS_PER_YEAR)\n",
    "\n",
    "def sharpe_ratio(daily_returns, risk_free_annual=0.0):\n",
    "    \"\"\"Annualized Sharpe using daily returns.\n",
    "    risk_free_annual is annual rate (e.g., 0.02 for 2%).\"\"\"\n",
    "    rf_daily = (1 + risk_free_annual)**(1/BUSINESS_DAYS_PER_YEAR) - 1\n",
    "    excess = daily_returns - rf_daily\n",
    "    mean_excess = np.nanmean(excess)\n",
    "    std = np.nanstd(daily_returns, ddof=0)\n",
    "    if std == 0 or np.isnan(std):\n",
    "        return np.nan\n",
    "    daily_sr = mean_excess / std\n",
    "    return daily_sr * np.sqrt(BUSINESS_DAYS_PER_YEAR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59919a00",
   "metadata": {},
   "source": [
    "## 1) Portfolio basics: weights, returns, and value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a327f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Simulate 5 years of daily prices for 4 assets ---\n",
    "n_years = 5\n",
    "n_days = BUSINESS_DAYS_PER_YEAR * n_years\n",
    "dates = pd.bdate_range(\"2012-01-02\", periods=n_days)\n",
    "\n",
    "# GBM parameters per asset (mu: drift, sigma: volatility)\n",
    "mus = np.array([0.18, 0.10, 0.06, 0.22])        # annualized drift (illustrative)\n",
    "sigmas = np.array([0.30, 0.22, 0.18, 0.35])     # annualized vol\n",
    "starts = np.array([150.0, 25.0, 120.0, 180.0])\n",
    "\n",
    "# Convert annual params to daily\n",
    "mu_d = mus / BUSINESS_DAYS_PER_YEAR\n",
    "sigma_d = sigmas / np.sqrt(BUSINESS_DAYS_PER_YEAR)\n",
    "\n",
    "prices = {}\n",
    "for i, name in enumerate([\"AAPL\",\"CSCO\",\"IBM\",\"AMZN\"]):\n",
    "    shocks = rng.normal(0, 1, size=n_days)\n",
    "    increments = (mu_d[i] - 0.5 * sigma_d[i]**2) + sigma_d[i]*shocks\n",
    "    path = starts[i] * np.exp(np.cumsum(increments))\n",
    "    prices[name] = path\n",
    "\n",
    "prices = pd.DataFrame(prices, index=dates)\n",
    "\n",
    "# Normalize series to start at 1\n",
    "norm = prices / prices.iloc[0]\n",
    "\n",
    "# Pick weights (must sum to 1)\n",
    "weights = pd.Series([0.30, 0.20, 0.10, 0.40], index=prices.columns)\n",
    "\n",
    "# Compute daily (arithmetic) returns\n",
    "daily_ret = prices.pct_change().dropna()\n",
    "\n",
    "# Portfolio daily return = weighted sum of asset daily returns\n",
    "port_daily = (daily_ret * weights).sum(axis=1)\n",
    "\n",
    "# Cumulative return from $1 initial (i.e., growth of 1)\n",
    "port_cum = (1 + port_daily).cumprod()\n",
    "\n",
    "# Build positions from an initial capital\n",
    "initial_capital = 1_000_000.0\n",
    "position_values = norm.mul(weights, axis=1) * initial_capital  # allocation by normalized prices\n",
    "total_value = position_values.sum(axis=1)\n",
    "\n",
    "newfig(); total_value.plot(); plt.title(\"Total Portfolio Value (simulated)\"); plt.xlabel(\"Date\"); plt.ylabel(\"USD\"); plt.show()\n",
    "\n",
    "newfig(); position_values.plot(); plt.title(\"Position Values by Asset\"); plt.xlabel(\"Date\"); plt.ylabel(\"USD\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a69ae4",
   "metadata": {},
   "source": [
    "### Exercise 1 (Solved): Change weights and recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc3836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SOLUTION\n",
    "eq_weights = pd.Series([0.25,0.25,0.25,0.25], index=prices.columns)\n",
    "eq_port_daily = (daily_ret * eq_weights).sum(axis=1)\n",
    "eq_total_value = (norm.mul(eq_weights, axis=1) * initial_capital).sum(axis=1)\n",
    "\n",
    "newfig(); eq_total_value.plot(); plt.title(\"Total Value with Equal Weights\"); plt.xlabel(\"Date\"); plt.ylabel(\"USD\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa601b",
   "metadata": {},
   "source": [
    "## 2) Sharpe Ratio: risk-adjusted performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e787d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_daily = port_daily.mean()\n",
    "std_daily  = port_daily.std(ddof=0)\n",
    "\n",
    "ann_ret = annualize_return(mean_daily)\n",
    "ann_vol = annualize_vol(std_daily)\n",
    "ann_sr  = sharpe_ratio(port_daily, risk_free_annual=0.0)\n",
    "\n",
    "print(f\"Annualized return: {ann_ret:.2%}\")\n",
    "print(f\"Annualized volatility: {ann_vol:.2%}\")\n",
    "print(f\"Annualized Sharpe (rf=0%): {ann_sr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67794ee",
   "metadata": {},
   "source": [
    "### Exercise 2 (Solved): Non‑zero risk‑free rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb225ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sr_2pct = sharpe_ratio(port_daily, risk_free_annual=0.02)\n",
    "print(f\"Annualized Sharpe (rf=2%): {sr_2pct:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e155c896",
   "metadata": {},
   "source": [
    "## 3) Log vs arithmetic returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9028cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arith = prices.pct_change().dropna()\n",
    "logret = np.log(prices / prices.shift(1)).dropna()\n",
    "\n",
    "display(arith.head(3))\n",
    "display(logret.head(3))\n",
    "\n",
    "newfig(); arith[\"AAPL\"].hist(bins=80); plt.title(\"Arithmetic Daily Returns (AAPL)\"); plt.xlabel(\"Return\"); plt.ylabel(\"Freq\"); plt.show()\n",
    "newfig(); logret[\"AAPL\"].hist(bins=80); plt.title(\"Log Daily Returns (AAPL)\"); plt.xlabel(\"Log Return\"); plt.ylabel(\"Freq\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12a2f8",
   "metadata": {},
   "source": [
    "## 4) Monte Carlo allocations and maximum Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precompute from log returns\n",
    "mu_log = logret.mean() * BUSINESS_DAYS_PER_YEAR               # approx annualized\n",
    "cov_log = logret.cov() * BUSINESS_DAYS_PER_YEAR               # annualized covariance\n",
    "\n",
    "def rand_weights(n, rng):\n",
    "    w = rng.random(n)\n",
    "    return w / w.sum()\n",
    "\n",
    "def portfolio_return_vol(weights, mu, cov):\n",
    "    ann_ret = float(np.dot(weights, mu))\n",
    "    ann_vol = float(np.sqrt(weights @ cov.values @ weights))\n",
    "    return ann_ret, ann_vol\n",
    "\n",
    "N = 8000\n",
    "rets = np.empty(N)\n",
    "vols = np.empty(N)\n",
    "srs  = np.empty(N)\n",
    "W    = np.empty((N, len(mu_log)))\n",
    "\n",
    "for i in range(N):\n",
    "    w = rand_weights(len(mu_log), rng)\n",
    "    r, v = portfolio_return_vol(w, mu_log.values, cov_log)\n",
    "    rets[i] = r\n",
    "    vols[i] = v\n",
    "    srs[i]  = r / v if v > 0 else np.nan\n",
    "    W[i] = w\n",
    "\n",
    "best_idx = np.nanargmax(srs)\n",
    "best_w   = W[best_idx]\n",
    "best_r, best_v = rets[best_idx], vols[best_idx]\n",
    "best_sr  = srs[best_idx]\n",
    "\n",
    "print(\"Best random allocation (weights):\")\n",
    "print(pd.Series(best_w, index=prices.columns).round(3))\n",
    "print(f\"Best Sharpe ~ {best_sr:.3f} | ann return {best_r:.2%} | ann vol {best_v:.2%}\")\n",
    "\n",
    "newfig()\n",
    "plt.scatter(vols, rets, c=srs)  # default colormap\n",
    "plt.scatter([best_v], [best_r], s=120, edgecolors=\"black\")\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Annualized Return\")\n",
    "plt.title(\"Monte Carlo Portfolios (colored by Sharpe)\")\n",
    "plt.colorbar(label=\"Sharpe\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d254c587",
   "metadata": {},
   "source": [
    "## 5) Optimization & Efficient Frontier (Markowitz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827cf855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from scipy.optimize import minimize\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "\n",
    "bounds = [(0.0, 1.0)] * len(mu_log)\n",
    "\n",
    "def cons_sum_to_1(w):\n",
    "    return np.sum(w) - 1.0\n",
    "\n",
    "constraints_sum = ({'type':'eq', 'fun': cons_sum_to_1},)\n",
    "\n",
    "def neg_sharpe(w, mu, cov, rf=0.0):\n",
    "    r, v = portfolio_return_vol(w, mu, cov)\n",
    "    ex_r = r - rf\n",
    "    return -ex_r / v if v > 0 else 1e9\n",
    "\n",
    "w0 = np.array([1.0/len(mu_log)]*len(mu_log))\n",
    "\n",
    "if SCIPY_OK:\n",
    "    res = minimize(neg_sharpe, w0, args=(mu_log.values, cov_log.values, 0.0),\n",
    "                   method='SLSQP', bounds=bounds, constraints=constraints_sum)\n",
    "    opt_w = res.x\n",
    "else:\n",
    "    trials = 50000\n",
    "    best = (None, -np.inf)\n",
    "    for _ in range(trials):\n",
    "        w = rand_weights(len(mu_log), rng)\n",
    "        r, v = portfolio_return_vol(w, mu_log.values, cov_log.values)\n",
    "        sr = (r / v) if v>0 else -np.inf\n",
    "        if sr > best[1]:\n",
    "            best = (w, sr)\n",
    "    opt_w = best[0]\n",
    "\n",
    "opt_r, opt_v = portfolio_return_vol(opt_w, mu_log.values, cov_log.values)\n",
    "opt_sr = opt_r / opt_v\n",
    "print(\"Optimized weights (max Sharpe):\")\n",
    "print(pd.Series(opt_w, index=prices.columns).round(3))\n",
    "print(f\"Sharpe ~ {opt_sr:.3f} | ann return {opt_r:.2%} | ann vol {opt_v:.2%}\")\n",
    "\n",
    "# Efficient frontier\n",
    "targets = np.linspace(rets.min()*0.9, rets.max()*1.05, 50)\n",
    "front_vol = []\n",
    "\n",
    "if SCIPY_OK:\n",
    "    for tr in targets:\n",
    "        cons = (\n",
    "            {'type':'eq', 'fun': cons_sum_to_1},\n",
    "            {'type':'eq', 'fun': lambda w, mu=mu_log.values: np.dot(w, mu) - tr}\n",
    "        )\n",
    "        def vol_objective(w, cov=cov_log.values):\n",
    "            return np.sqrt(w @ cov @ w)\n",
    "        res2 = minimize(vol_objective, w0, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "        front_vol.append(res2.fun if res2.success else np.nan)\n",
    "else:\n",
    "    for tr in targets:\n",
    "        best_v = np.inf\n",
    "        for _ in range(6000):\n",
    "            w = rand_weights(len(mu_log), rng)\n",
    "            r, v = portfolio_return_vol(w, mu_log.values, cov_log.values)\n",
    "            if abs(r - tr) < 0.003 and v < best_v:\n",
    "                best_v = v\n",
    "        front_vol.append(best_v if np.isfinite(best_v) else np.nan)\n",
    "\n",
    "newfig()\n",
    "plt.scatter(vols, rets, c=srs)\n",
    "plt.scatter([opt_v], [opt_r], s=140, edgecolors=\"black\")\n",
    "plt.plot(front_vol, targets, linestyle=\"--\", linewidth=2.5)\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Annualized Return\")\n",
    "plt.title(\"Efficient Frontier and Max-Sharpe Portfolio\")\n",
    "plt.colorbar(label=\"Sharpe\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe964f",
   "metadata": {},
   "source": [
    "### Exercise 3 (Solved): Add a no‑AMZN constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if SCIPY_OK:\n",
    "    idx_amzn = list(mu_log.index).index(\"AMZN\")\n",
    "    bounds_no_amzn = list(bounds)\n",
    "    bounds_no_amzn[idx_amzn] = (0.0, 0.0)  # fix to zero\n",
    "\n",
    "    res3 = minimize(neg_sharpe, w0, args=(mu_log.values, cov_log.values, 0.0),\n",
    "                    method='SLSQP', bounds=bounds_no_amzn, constraints=constraints_sum)\n",
    "    w_no_amzn = res3.x\n",
    "else:\n",
    "    idx_amzn = list(mu_log.index).index(\"AMZN\")\n",
    "    best = (None, -np.inf)\n",
    "    for _ in range(60000):\n",
    "        w = rand_weights(len(mu_log), rng)\n",
    "        w[idx_amzn] = 0.0\n",
    "        w = w / w.sum()\n",
    "        r, v = portfolio_return_vol(w, mu_log.values, cov_log.values)\n",
    "        sr = (r / v) if v>0 else -np.inf\n",
    "        if sr > best[1]:\n",
    "            best = (w, sr)\n",
    "    w_no_amzn = best[0]\n",
    "\n",
    "r3, v3 = portfolio_return_vol(w_no_amzn, mu_log.values, cov_log.values)\n",
    "sr3 = r3 / v3\n",
    "print(\"Max-Sharpe with AMZN excluded:\")\n",
    "print(pd.Series(w_no_amzn, index=prices.columns).round(3))\n",
    "print(f\"Sharpe ~ {sr3:.3f} | ann return {r3:.2%} | ann vol {v3:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f632a8",
   "metadata": {},
   "source": [
    "## 6) Funds 101: ETFs, mutual funds, hedge funds (quick notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180b58e",
   "metadata": {},
   "source": [
    "\n",
    "- **ETFs**: tradable like stocks; transparent baskets; typically low **expense ratios** (≈ 0.01–1%). High liquidity.\n",
    "- **Mutual funds**: pooled, managed per prospectus; disclose holdings (e.g., quarterly); higher expense ratios (≈ 0.5–3%+). Trades typically priced end‑of‑day.\n",
    "- **Hedge funds**: pooled, often **accredited investors only**; broad strategies (leverage/derivatives) with fee structures like **2 and 20** (or variants). Lower transparency/liquidity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74963d5",
   "metadata": {},
   "source": [
    "## 7) Order books and market microstructure (conceptual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9cf68e",
   "metadata": {},
   "source": [
    "\n",
    "**Order basics**: An order includes side (buy/sell), symbol, quantity, and price **type**:\n",
    "- **Market**: execute at best available price.\n",
    "- **Limit**: execute at a specified price or better (buy ≤ limit, sell ≥ limit).\n",
    "\n",
    "**Order book**: The exchange maintains aggregated **bids** (buys) and **asks** (sells). A buy order matches the **lowest asks** first; large orders can fill at **multiple prices**.\n",
    "\n",
    "**Routing**: Your broker may route internally, to exchanges, or to **dark pools** (private venues). By regulation, you must get at least the **NBBO** (best available prices) for execution.\n",
    "\n",
    "**HFT (high-frequency trading)**: Firms colocate servers near exchange engines and exploit **latency** differences (microseconds) for tiny, repeated edge; generally irrelevant to small retail orders, more relevant to large, multi‑venue institutional flows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa72a5",
   "metadata": {},
   "source": [
    "## 8) Short selling — profit from price declines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def long_short_pnl(price_path, side=\"long\", qty=100):\n",
    "    price_path = np.asarray(price_path, dtype=float)\n",
    "    if side == \"long\":\n",
    "        pnl = qty*(price_path - price_path[0])\n",
    "    else:  # short\n",
    "        pnl = qty*(price_path[0] - price_path)\n",
    "    return pnl\n",
    "\n",
    "# Build a small path with a drop then rise\n",
    "path = np.array([500, 520, 480, 450, 470, 510], dtype=float)\n",
    "pnl_long = long_short_pnl(path, \"long\", 10)\n",
    "pnl_short= long_short_pnl(path, \"short\",10)\n",
    "\n",
    "print(\"Prices:   \", path.tolist())\n",
    "print(\"Long PnL: \", pnl_long.tolist())\n",
    "print(\"Short PnL:\", pnl_short.tolist())\n",
    "\n",
    "newfig(); plt.plot(path); plt.title(\"Example Price Path\"); plt.xlabel(\"Step\"); plt.ylabel(\"Price\"); plt.show()\n",
    "newfig(); plt.plot(pnl_long); plt.title(\"Long P&L (qty=10)\"); plt.xlabel(\"Step\"); plt.ylabel(\"PnL\"); plt.show()\n",
    "newfig(); plt.plot(pnl_short); plt.title(\"Short P&L (qty=10)\"); plt.xlabel(\"Step\"); plt.ylabel(\"PnL\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45005b22",
   "metadata": {},
   "source": [
    "## 9) CAPM — beta & alpha via simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make daily market returns (normal-ish) and stock returns with known beta & alpha\n",
    "T = 1000\n",
    "market_ret = rng.normal(0.0004, 0.01, size=T)            # daily mean ~4 bps, vol ~1%\n",
    "true_beta  = 1.2\n",
    "true_alpha = 0.0002\n",
    "eps        = rng.normal(0.0, 0.008, size=T)              # idiosyncratic\n",
    "stock_ret  = true_alpha + true_beta*market_ret + eps\n",
    "\n",
    "# Fit regression: stock_ret ~ a + b * market_ret\n",
    "b, a = np.polyfit(market_ret, stock_ret, 1)  # returns slope, intercept\n",
    "\n",
    "print(f\"True beta={true_beta:.2f}, est beta={b:.3f}\")\n",
    "print(f\"True alpha={true_alpha:.5f}, est alpha={a:.5f}\")\n",
    "\n",
    "# R^2\n",
    "yhat = a + b*market_ret\n",
    "ss_res = np.sum((stock_ret - yhat)**2)\n",
    "ss_tot = np.sum((stock_ret - np.mean(stock_ret))**2)\n",
    "r2 = 1 - ss_res/ss_tot\n",
    "print(f\"R^2 = {r2:.3f}\")\n",
    "\n",
    "newfig(); plt.scatter(market_ret, stock_ret, alpha=0.25)\n",
    "plt.plot(np.sort(market_ret), a + b*np.sort(market_ret))\n",
    "plt.title(\"CAPM: Stock vs Market (synthetic)\")\n",
    "plt.xlabel(\"Market daily return\")\n",
    "plt.ylabel(\"Stock daily return\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78002b4",
   "metadata": {},
   "source": [
    "## 10) Stock splits & dividends — use *adjusted* prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742edb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a toy price then a 2-for-1 split on day 50\n",
    "toy = pd.Series(100 + np.cumsum(rng.normal(0, 1, size=100)))\n",
    "raw = toy.copy()\n",
    "raw.iloc[50:] = raw.iloc[50:] / 2.0  # split halves the quoted price\n",
    "\n",
    "# Adjusted series rescales prior history by 0.5 so the line is continuous\n",
    "adj = toy.copy()\n",
    "adj.iloc[:50] = adj.iloc[:50] / 2.0\n",
    "\n",
    "newfig(); plt.plot(raw.values); plt.title(\"Raw Price with 2-for-1 Split\"); plt.xlabel(\"Day\"); plt.ylabel(\"Price\"); plt.show()\n",
    "newfig(); plt.plot(adj.values); plt.title(\"Adjusted Price (split preserved)\"); plt.xlabel(\"Day\"); plt.ylabel(\"Adjusted Price\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645151bb",
   "metadata": {},
   "source": [
    "## 11) Survivorship bias (concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e8781",
   "metadata": {},
   "source": [
    "\n",
    "Benchmarks like the S&P 500 **change membership** over time. Backtests using the **current** constituents far in the past **overstate** performance (failed firms are missing). For benchmark‑dependent strategies, consider **survivorship‑bias‑free** constituents for the historical period under test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc670a5",
   "metadata": {},
   "source": [
    "## Appendix — convenience helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898fe64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_weights(w, names=None, title=None):\n",
    "    s = pd.Series(w, index=names if names is not None else range(len(w)))\n",
    "    if title:\n",
    "        print(title)\n",
    "    print(s.sort_values(ascending=False).round(3))\n",
    "\n",
    "# Example\n",
    "# show_weights(opt_w, names=prices.columns, title=\"Optimized weights (max Sharpe):\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
